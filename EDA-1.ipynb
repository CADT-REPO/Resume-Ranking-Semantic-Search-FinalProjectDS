{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries such as pandas, numpy, matplotlib, seaborn, and wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load the dataset from a CSV file into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = 'data/Resume.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "Display the first few rows of the dataset and get basic information such as shape, columns, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "#del df['ID']\n",
    "# del df['Resume_html']\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the DataFrame\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for textual data\n",
    "df['Resume_str_length'] = df['Resume_str'].apply(len)\n",
    "df['Resume_html_length'] = df['Resume_html'].apply(len)\n",
    "\n",
    "# Category distribution\n",
    "category_distribution = df['Category'].value_counts()\n",
    "\n",
    "# Summary of textual lengths\n",
    "resume_str_summary = df['Resume_str_length'].describe()\n",
    "resume_html_summary = df['Resume_html_length'].describe()\n",
    "\n",
    "category_distribution, resume_str_summary, resume_html_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Analysis\n",
    "Check for missing values in the dataset and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing\n",
    "Preprocess the text data by removing punctuation, stop words, and performing tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Text Cleaning \n",
    "# Remove numeric data, punctuation, and redundant spaces.\n",
    "# Handle lemmatization or stemming for reducing words to their base forms.\n",
    "# Function to clean the resume text\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('#\\S+', '', text)  # remove hashtags\n",
    "    text = re.sub('@\\S+', '  ', text)  # remove mentions\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n",
    "\n",
    "     # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "   # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "    \n",
    "\n",
    "# # preprocessing text\n",
    "# df['Resume'] = df['Resume'].apply(lambda w: preprocess(w))\n",
    "\n",
    "# Apply cleaning to the Resume_str column\n",
    "df['Cleaned_Resume_str'] = df['Resume_str'].apply(clean_text)\n",
    "\n",
    "# Display a sample of the cleaned text\n",
    "df[['Resume_str', 'Cleaned_Resume_str']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Categorical Encoding\n",
    "\n",
    "\n",
    "# Encode the Category column\n",
    "label_encoder = LabelEncoder()\n",
    "df['Category_Encoded'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "# Display the first few rows to see the encoded categories\n",
    "df[['Category', 'Category_Encoded']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all categories\n",
    "categories = np.sort(df['Category'].unique())\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df for corpus and category\n",
    "df_categories = [df[df['Category'] == category].loc[:, ['Resume_str', 'Category']] for category in categories]\n",
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data based on the 'Cleaned_Resume' column\n",
    "duplicates = df[df.duplicated(subset=['Cleaned_Resume_str'])]\n",
    "\n",
    "# Display the number of duplicates and some of the duplicated rows\n",
    "duplicates_count = duplicates.shape[0]\n",
    "if duplicates_count > 0:\n",
    "    print(f\"There are {duplicates_count} duplicate rows based on the 'Cleaned_Resume' column.\")\n",
    "    print(duplicates.head())  # Display a few rows with duplicates\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_data = df.drop_duplicates(subset=['Cleaned_Resume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Perform EDA on the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_mod = ['Category']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the distribution of job categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='Category', order=df['Category'].value_counts().index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of Job Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all resumes into a single string\n",
    "text = ' '.join(df['Cleaned_Resume_str'])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Resumes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the length of each resume\n",
    "df['resume_length'] = df['Cleaned_Resume_str'].apply(len)\n",
    "\n",
    "# Plot the distribution of resume lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['resume_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Resume Lengths')\n",
    "plt.xlabel('Resume Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to plot N-grams\n",
    "def plot_ngrams(text_data, ngram_range=(1, 1), num=20):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    ngrams = vectorizer.fit_transform(text_data)\n",
    "    ngrams_sum = ngrams.sum(axis=0)\n",
    "    ngrams_freq = [(word, ngrams_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    ngrams_freq = sorted(ngrams_freq, key=lambda x: x[1], reverse=True)[:num]\n",
    "    \n",
    "    ngrams_df = pd.DataFrame(ngrams_freq, columns=['N-gram', 'Frequency'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Frequency', y='N-gram', data=ngrams_df)\n",
    "    plt.title(f'Top {num} N-grams')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('N-gram')\n",
    "    plt.show()\n",
    "\n",
    "# Plot top unigrams (single words)\n",
    "plot_ngrams(df['Cleaned_Resume_str'], ngram_range=(1, 1), num=20)\n",
    "\n",
    "# Plot top bigrams (two-word phrases)\n",
    "plot_ngrams(df['Cleaned_Resume_str'], ngram_range=(2, 2), num=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
